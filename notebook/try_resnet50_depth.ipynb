{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import models, datasets, tv_tensors\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights \n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from transformers import pipeline\n",
    "\n",
    "from torchvision.models.detection.rpn import concat_box_prediction_layers\n",
    "from torchvision.models.detection.roi_heads import fastrcnn_loss\n",
    "from torchvision.transforms.functional import to_pil_image, pil_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddDepth(torch.nn.Module):\n",
    "    def forward(self, image, label):\n",
    "        pipe = pipeline(task=\"depth-estimation\",\n",
    "                    model=\"LiheYoung/depth-anything-small-hf\")\n",
    "        depth = pipe(to_pil_image(image))[\"depth\"]\n",
    "        depth_tensor = pil_to_tensor(depth)\n",
    "        rgbd = torch.concatenate([image, depth_tensor], dim=0)\n",
    "        return rgbd, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToDepth:\n",
    "    def __call__(self, image):\n",
    "        pipe = pipeline(task=\"depth-estimation\",\n",
    "                        model=\"LiheYoung/depth-anything-small-hf\")\n",
    "        depth = pipe(image)[\"depth\"]\n",
    "        depth_tensor = pil_to_tensor(depth)\n",
    "        rgbd = torch.concatenate([pil_to_tensor(image), depth_tensor], dim=0)\n",
    "        return rgbd\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = weights.transforms()\n",
    "transform = transforms.Compose([\n",
    "    # AddDepth()\n",
    "    ToDepth()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=9.35s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Train samples size: 118287\n",
      "\n",
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Validation samples size: 5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CocoDetection(root=\"../data/coco/train2017\", \n",
    "                                       annFile=\"../data/coco/annotations/instances_train2017.json\",\n",
    "                                       transform=transform)\n",
    "train_dataset = datasets.wrap_dataset_for_transforms_v2(train_dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])\n",
    "print(\"\\nTrain samples size:\", len(train_dataset), end='\\n\\n')\n",
    "\n",
    "val_dataset = datasets.CocoDetection(root=\"../data/coco/val2017\", \n",
    "                                     annFile=\"../data/coco/annotations/instances_val2017.json\",\n",
    "                                     transform=transform)\n",
    "val_dataset = datasets.wrap_dataset_for_transforms_v2(val_dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])                                     \n",
    "print(\"\\nValidation samples size:\", len(val_dataset), end='\\n\\n')\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda batch: tuple(zip(*batch)))\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=lambda batch: tuple(zip(*batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_forward(model, images, targets):\n",
    "    # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images (list[Tensor]): images to be processed\n",
    "        targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "    Returns:\n",
    "        result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "            It returns list[BoxList] contains additional fields\n",
    "            like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    original_image_sizes: List[Tuple[int, int]] = []\n",
    "    for img in images:\n",
    "        val = img.shape[-2:]\n",
    "        assert len(val) == 2\n",
    "        original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "    images, targets = model.transform(images, targets)\n",
    "\n",
    "    # Check for degenerate boxes\n",
    "    # TODO: Move this to a function\n",
    "    if targets is not None:\n",
    "        for target_idx, target in enumerate(targets):\n",
    "            boxes = target[\"boxes\"]\n",
    "            degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
    "            if degenerate_boxes.any():\n",
    "                # print the first degenerate box\n",
    "                bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
    "                degen_bb: List[float] = boxes[bb_idx].tolist()\n",
    "                raise ValueError(\n",
    "                    \"All bounding boxes should have positive height and width.\"\n",
    "                    f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n",
    "                )\n",
    "\n",
    "    features = model.backbone(images.tensors)\n",
    "    if isinstance(features, torch.Tensor):\n",
    "        features = OrderedDict([(\"0\", features)])\n",
    "    model.rpn.training = True\n",
    "    # model.roi_heads.training=True\n",
    "\n",
    "    # proposals, proposal_losses = model.rpn(images, features, targets)\n",
    "    features_rpn = list(features.values())\n",
    "    objectness, pred_bbox_deltas = model.rpn.head(features_rpn)\n",
    "    anchors = model.rpn.anchor_generator(images, features_rpn)\n",
    "\n",
    "    num_images = len(anchors)\n",
    "    num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n",
    "    num_anchors_per_level = [s[0] * s[1] * s[2]\n",
    "                             for s in num_anchors_per_level_shape_tensors]\n",
    "    objectness, pred_bbox_deltas = concat_box_prediction_layers(\n",
    "        objectness, pred_bbox_deltas)\n",
    "    # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n",
    "    # note that we detach the deltas because Faster R-CNN do not backprop through\n",
    "    # the proposals\n",
    "    proposals = model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
    "    proposals = proposals.view(num_images, -1, 4)\n",
    "    proposals, scores = model.rpn.filter_proposals(\n",
    "        proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
    "\n",
    "    proposal_losses = {}\n",
    "    assert targets is not None\n",
    "    labels, matched_gt_boxes = model.rpn.assign_targets_to_anchors(\n",
    "        anchors, targets)\n",
    "    regression_targets = model.rpn.box_coder.encode(matched_gt_boxes, anchors)\n",
    "    loss_objectness, loss_rpn_box_reg = model.rpn.compute_loss(\n",
    "        objectness, pred_bbox_deltas, labels, regression_targets\n",
    "    )\n",
    "    proposal_losses = {\n",
    "        \"loss_objectness\": loss_objectness,\n",
    "        \"loss_rpn_box_reg\": loss_rpn_box_reg,\n",
    "    }\n",
    "\n",
    "    # detections, detector_losses = model.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "    image_shapes = images.image_sizes\n",
    "    proposals, matched_idxs, labels, regression_targets = model.roi_heads.select_training_samples(\n",
    "        proposals, targets)\n",
    "    box_features = model.roi_heads.box_roi_pool(\n",
    "        features, proposals, image_shapes)\n",
    "    box_features = model.roi_heads.box_head(box_features)\n",
    "    class_logits, box_regression = model.roi_heads.box_predictor(box_features)\n",
    "\n",
    "    result: List[Dict[str, torch.Tensor]] = []\n",
    "    detector_losses = {}\n",
    "    loss_classifier, loss_box_reg = fastrcnn_loss(\n",
    "        class_logits, box_regression, labels, regression_targets)\n",
    "    detector_losses = {\"loss_classifier\": loss_classifier,\n",
    "                       \"loss_box_reg\": loss_box_reg}\n",
    "    boxes, scores, labels = model.roi_heads.postprocess_detections(\n",
    "        class_logits, box_regression, proposals, image_shapes)\n",
    "    num_images = len(boxes)\n",
    "    for i in range(num_images):\n",
    "        result.append(\n",
    "            {\n",
    "                \"boxes\": boxes[i],\n",
    "                \"labels\": labels[i],\n",
    "                \"scores\": scores[i],\n",
    "            }\n",
    "        )\n",
    "    detections = result\n",
    "    detections = model.transform.postprocess(\n",
    "        detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n",
    "    model.rpn.training = False\n",
    "    model.roi_heads.training = False\n",
    "    losses = {}\n",
    "    losses.update(detector_losses)\n",
    "    losses.update(proposal_losses)\n",
    "    return losses, detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_images_with_labels(dataset):\n",
    "#     filtered_indices = []\n",
    "#     for idx in range(len(dataset)):\n",
    "#         _, ann = dataset[idx]\n",
    "#         if ('boxes' in ann) and ('labels' in ann):  # If annotations exist\n",
    "#             filtered_indices.append(idx)\n",
    "#     return filtered_indices\n",
    "\n",
    "# len(filter_images_with_labels(train_dataset_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"\\nAvailable device:\", device)\n",
    "\n",
    "# weights = ResNet50_Weights.IMAGENET1K_V1\n",
    "# model = resnet50(weights=weights)\n",
    "# model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1)\n",
    "model = models.get_model(\"maskrcnn_resnet50_fpn_v2\", weights=None, weights_backbone=None).train()\n",
    "\n",
    "model_depth = models.get_model(\"maskrcnn_resnet50_fpn_v2\", weights=None, weights_backbone=None).train()\n",
    "model_depth.backbone.body.conv1 = torch.nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      5\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model_depth \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_depth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      9\u001b[0m     model_depth\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Documents/Repos/depth2detect/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Repos/depth2detect/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Repos/depth2detect/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Repos/depth2detect/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Repos/depth2detect/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/Documents/Repos/depth2detect/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "params = [p for p in model_depth.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "model_depth = model_depth.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_depth.train()\n",
    "    total_train_loss = 0\n",
    "    for images, targets in train_loader:\n",
    "        if 'boxes' not in targets[0].keys():\n",
    "            break\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        images = [image.to(device) for image in images]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model_depth(images, targets)\n",
    "\n",
    "        del images\n",
    "        del targets\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += losses.item()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    model_depth.eval()\n",
    "    total_val_loss = 0\n",
    "    for images, targets in val_loader:\n",
    "        if 'boxes' not in targets[0].keys():\n",
    "            break\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        images = [image.to(device) for image in images]\n",
    "        # loss_dict = model_depth(images, targets)\n",
    "        losses, predictions = eval_forward(model_depth, images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        total_val_loss += losses.item()\n",
    "\n",
    "        del images\n",
    "        del targets\n",
    "        \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} || Train Loss: {avg_train_loss} || Val Loss: {avg_val_loss}\")\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, targets in val_loader:\n",
    "    images = [image.to(device) for image in images]\n",
    "    if 'boxes' not in targets[0].keys():\n",
    "        break\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss_classifier': tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       "  'loss_box_reg': tensor(0.0131, device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  'loss_objectness': tensor(1.8196, device='cuda:0',\n",
       "         grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  'loss_rpn_box_reg': tensor(5.6260, device='cuda:0', grad_fn=<DivBackward0>)},\n",
       " [{'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward0>),\n",
       "   'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "   'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward0>)},\n",
       "  {'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward0>),\n",
       "   'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "   'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward0>)}])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses, predictions = eval_forward(model, images, targets)\n",
    "losses = sum(loss for loss in loss_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_forward(model, images, targets):\n",
    "    # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images (list[Tensor]): images to be processed\n",
    "        targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "    Returns:\n",
    "        result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "            It returns list[BoxList] contains additional fields\n",
    "            like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    original_image_sizes: List[Tuple[int, int]] = []\n",
    "    for img in images:\n",
    "        val = img.shape[-2:]\n",
    "        assert len(val) == 2\n",
    "        original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "    images, targets = model.transform(images, targets)\n",
    "\n",
    "    # Check for degenerate boxes\n",
    "    # TODO: Move this to a function\n",
    "    if targets is not None:\n",
    "        for target_idx, target in enumerate(targets):\n",
    "            boxes = target[\"boxes\"]\n",
    "            degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
    "            if degenerate_boxes.any():\n",
    "                # print the first degenerate box\n",
    "                bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
    "                degen_bb: List[float] = boxes[bb_idx].tolist()\n",
    "                raise ValueError(\n",
    "                    \"All bounding boxes should have positive height and width.\"\n",
    "                    f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n",
    "                )\n",
    "\n",
    "    features = model.backbone(images.tensors)\n",
    "    if isinstance(features, torch.Tensor):\n",
    "        features = OrderedDict([(\"0\", features)])\n",
    "    model.rpn.training=True\n",
    "    #model.roi_heads.training=True\n",
    "\n",
    "\n",
    "    #####proposals, proposal_losses = model.rpn(images, features, targets)\n",
    "    features_rpn = list(features.values())\n",
    "    objectness, pred_bbox_deltas = model.rpn.head(features_rpn)\n",
    "    anchors = model.rpn.anchor_generator(images, features_rpn)\n",
    "\n",
    "    num_images = len(anchors)\n",
    "    num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n",
    "    num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n",
    "    objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n",
    "    # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n",
    "    # note that we detach the deltas because Faster R-CNN do not backprop through\n",
    "    # the proposals\n",
    "    proposals = model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
    "    proposals = proposals.view(num_images, -1, 4)\n",
    "    proposals, scores = model.rpn.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
    "\n",
    "    proposal_losses = {}\n",
    "    assert targets is not None\n",
    "    labels, matched_gt_boxes = model.rpn.assign_targets_to_anchors(anchors, targets)\n",
    "    regression_targets = model.rpn.box_coder.encode(matched_gt_boxes, anchors)\n",
    "    loss_objectness, loss_rpn_box_reg = model.rpn.compute_loss(\n",
    "        objectness, pred_bbox_deltas, labels, regression_targets\n",
    "    )\n",
    "    proposal_losses = {\n",
    "        \"loss_objectness\": loss_objectness,\n",
    "        \"loss_rpn_box_reg\": loss_rpn_box_reg,\n",
    "    }\n",
    "\n",
    "    #####detections, detector_losses = model.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "    image_shapes = images.image_sizes\n",
    "    proposals, matched_idxs, labels, regression_targets = model.roi_heads.select_training_samples(proposals, targets)\n",
    "    box_features = model.roi_heads.box_roi_pool(features, proposals, image_shapes)\n",
    "    box_features = model.roi_heads.box_head(box_features)\n",
    "    class_logits, box_regression = model.roi_heads.box_predictor(box_features)\n",
    "\n",
    "    result: List[Dict[str, torch.Tensor]] = []\n",
    "    detector_losses = {}\n",
    "    loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n",
    "    detector_losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n",
    "    boxes, scores, labels = model.roi_heads.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n",
    "    num_images = len(boxes)\n",
    "    for i in range(num_images):\n",
    "        result.append(\n",
    "            {\n",
    "                \"boxes\": boxes[i],\n",
    "                \"labels\": labels[i],\n",
    "                \"scores\": scores[i],\n",
    "            }\n",
    "        )\n",
    "    detections = result\n",
    "    detections = model.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n",
    "    model.rpn.training=False\n",
    "    model.roi_heads.training=False\n",
    "    losses = {}\n",
    "    losses.update(detector_losses)\n",
    "    losses.update(proposal_losses)\n",
    "    return losses, detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(0.2635, grad_fn=<NllLossBackward0>),\n",
       " 'loss_box_reg': tensor(0.0336, grad_fn=<DivBackward0>),\n",
       " 'loss_objectness': tensor(10.7953, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'loss_rpn_box_reg': tensor(45.5220, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# images = tuple(image.to(device) for image in images)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extract bounding boxes and labels for each image in the batch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m boxes \u001b[38;5;241m=\u001b[39m [target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m [target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# images = tuple(image.to(device) for image in images)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extract bounding boxes and labels for each image in the batch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m boxes \u001b[38;5;241m=\u001b[39m [\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbbox\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m [target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "for images, targets in train_loader:\n",
    "    images = images.to(device)\n",
    "    # images = tuple(image.to(device) for image in images)\n",
    "\n",
    "    # Extract bounding boxes and labels for each image in the batch\n",
    "    boxes = [target['bbox'].to(device) for target in targets]\n",
    "    labels = [target['category_id'].to(device) for target in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, targets in train_loader:\n",
    "#     images = list(image.to(device) for image in images)\n",
    "#     targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#     print(type(images))\n",
    "#     print(images[0].shape)\n",
    "#     print(type(targets))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[[0],:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device).eval()\n",
    "for images, targets in train_loader:\n",
    "    images = images.to(device)\n",
    "    # targets = targets.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, targets)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCH):\n",
    "    for images, targets in train_loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ... (Calculate and print metrics, etc.) ...\n",
    "\n",
    "    # ... (Validation step) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120.07, 71.83, 134.49, 153.08]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data[1][3][0]['bbox'])\n",
    "    print(len(data[1][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
